{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-WR9J7YGbOOW"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fE6QG92bOOY"},"outputs":[],"source":["batch_size = 64\n","epochs = 30\n","latent_dim = 256\n","num_samples = 10000\n","data_path = 'data/fra-eng/fra.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYlPKBrabOOZ"},"outputs":[],"source":["# Собираем из текстов токены\n","\n","input_texts = []\n","target_texts = []\n","\n","input_vocab = set()\n","output_vocab = set()\n","\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, _ = line.split('\\t')\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    for word in input_text.split():\n","        input_vocab.add(word.strip())\n","    target_texts.append(target_text)\n","    for word in target_text.split():\n","        output_vocab.add(word.strip())\n","    \n","input_vocab2index = {word: i+2 for i, word in enumerate(input_vocab)}\n","output_vocab2index = {word: i+2 for i, word in enumerate(output_vocab)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f7p-xpqzbOOc"},"outputs":[],"source":["def indexesFromSentence(sentence, vocab):\n","    return [vocab[word.strip()] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(sentence, vocab):\n","    indexes = indexesFromSentence(sentence, vocab)\n","    indexes.append(1)\n","    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)\n","\n","\n","def tensorsFromSent(input_sentences, output_sentences):\n","    input_tensor = tensorFromSentence(input_sentences, input_vocab2index)\n","    target_tensor = tensorFromSentence(output_sentences, output_vocab2index)\n","    return (input_tensor, target_tensor)"]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    \n","    def __init__(self, dimensions):\n","        super(Attention, self).__init__()\n","\n","        self.attention_type = attention_type\n","        self.linear_in = nn.Linear(dimensions, dimensions, bias=False)\n","\n","        self.linear_out = nn.Linear(dimensions * 2, dimensions, bias=False)\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, query, context):\n","\n","        batch_size, output_len, dimensions = query.size()\n","        query_len = context.size(1)\n","\n","        query = query.reshape(batch_size * output_len, dimensions)\n","        query = self.linear_in(query)\n","        query = query.reshape(batch_size, output_len, dimensions)\n","\n","        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n","\n","        attention_scores = attention_scores.view(batch_size * output_len, query_len)\n","        attention_weights = self.softmax(attention_scores)\n","        attention_weights = attention_weights.view(batch_size, output_len, query_len)\n","\n","        mix = torch.bmm(attention_weights, context)\n","\n","        combined = torch.cat((mix, query), dim=2)\n","        combined = combined.view(batch_size * output_len, 2 * dimensions)\n","\n","        output = self.linear_out(combined).view(batch_size, output_len, dimensions)\n","        output = self.tanh(output)\n","\n","        return output, attention_weights\n"],"metadata":{"id":"YiRXjjgddGpa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["attn = Attention(1)"],"metadata":{"id":"ygdlOOZ1d2yT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A55ds341bOOd"},"outputs":[],"source":["class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(EncoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        output = embedded\n","        output, hidden = self.gru(output, hidden)\n","        return output, hidden\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size)\n","\n","\n","class AttnDecoderRNN(nn.Module):\n","    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=10):\n","        super(AttnDecoderRNN, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.dropout_p = dropout_p\n","        self.max_length = max_length\n","\n","        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n","        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n","        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n","        self.dropout = nn.Dropout(self.dropout_p)\n","        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n","        self.out = nn.Linear(self.hidden_size, self.output_size)\n","\n","    def forward(self, input, hidden, encoder_outputs):\n","        embedded = self.embedding(input).view(1, 1, -1)\n","        embedded = self.dropout(embedded)\n","\n","        attn_weights = F.softmax(\n","            #self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","            attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n","                                 encoder_outputs.unsqueeze(0))\n","\n","        output = torch.cat((embedded[0], attn_applied[0]), 1)\n","        output = self.attn_combine(output).unsqueeze(0)\n","\n","        #output = F.relu(output)\n","        output, hidden = self.gru(output, hidden)\n","\n","        output = F.log_softmax(self.out(output[0]), dim=1)\n","        return output, hidden, attn_weights\n","\n","    def initHidden(self):\n","        return torch.zeros(1, 1, self.hidden_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MyBlM7CWbOOf"},"outputs":[],"source":["def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=10):\n","    encoder_hidden = encoder.initHidden()\n","\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n","\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, encoder_hidden = encoder(\n","            input_tensor[ei], encoder_hidden)\n","        encoder_outputs[ei] = encoder_output[0, 0]\n","\n","    decoder_input = torch.tensor([[0]])\n","\n","    decoder_hidden = encoder_hidden\n","\n","    for di in range(target_length):\n","        decoder_output, decoder_hidden, decoder_attention = decoder(\n","            decoder_input, decoder_hidden, encoder_outputs)\n","        topv, topi = decoder_output.topk(1)\n","        decoder_input = topi.squeeze().detach()  # detach from history as input\n","\n","        loss += criterion(decoder_output, target_tensor[di])\n","        if decoder_input.item() == 1:\n","            break\n","\n","    loss.backward()\n","\n","    encoder_optimizer.step()\n","    decoder_optimizer.step()\n","\n","    return loss.item() / target_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_gzTwOlbOOh","outputId":"049bc83e-18bb-4fed-cedf-45c19ff57a9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(0 0%) 8.6994\n","(1 10%) 8.7353\n","(2 20%) 8.6483\n","(3 30%) 8.7025\n","(4 40%) 8.8337\n","(5 50%) 8.6862\n","(6 60%) 8.6198\n","(7 70%) 8.6412\n","(8 80%) 8.6065\n","(9 90%) 8.5707\n"]}],"source":["encoder = EncoderRNN(len(input_vocab2index)+2, 30)\n","attn_decoder1 = AttnDecoderRNN(30, len(output_vocab2index)+2, dropout_p=0.1)\n","\n","encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=0.01)\n","decoder_optimizer = torch.optim.SGD(attn_decoder1.parameters(), lr=0.01)\n","training_pairs = np.random.randint(0, len(input_texts), size=10)\n","criterion = nn.NLLLoss()\n","\n","print_loss_total = 0\n","for i in range(10):\n","    input_tensor, target_tensor = tensorsFromSent(input_texts[training_pairs[i]], target_texts[training_pairs[i]])\n","\n","    loss = train(input_tensor, target_tensor, encoder,\n","               attn_decoder1, encoder_optimizer, decoder_optimizer, criterion)\n","    print_loss_total += loss\n","    \n","    print_loss_avg = print_loss_total / 1\n","    print_loss_total = 0\n","    print('(%d %d%%) %.4f' % (i, i / 10 * 100, print_loss_avg))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"HW8.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}